{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b14fab19",
   "metadata": {
    "papermill": {
     "duration": 0.004985,
     "end_time": "2023-10-25T00:22:14.838676",
     "exception": false,
     "start_time": "2023-10-25T00:22:14.833691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Exercícios: Modelos para PLN\n",
    "Fonte: CARVALHO, Fabrício Galende Marques de. **Notas de aula da disciplina processamento de linguagem natural.** São José dos Campos, 2023.\n",
    "\n",
    "#### Atividade realizada em grupo: \n",
    "- Gabriel Camargo Leite\n",
    "- Giovana Thaís de O. Silva\n",
    "- Isabelle Dias R. Silva\n",
    "- João Marcos O. Santos\n",
    "- Maria Gabriela G. S. Reis\n",
    "- Thiago Henrique Ferreira"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acb7ca5",
   "metadata": {
    "papermill": {
     "duration": 0.004268,
     "end_time": "2023-10-25T00:22:14.849534",
     "exception": false,
     "start_time": "2023-10-25T00:22:14.845266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Terminologia e conceitos\n",
    "---\n",
    "\n",
    "1.  (TC.3.1) Qual tipo de problema pode surgir na montagem de um modelo do tipo bag of words caso etapas tais como a remoção de caracteres especiais (ex. sinais de pontuação) e conversão para minúsculas/maiúsculas não sejam efetuadas? Ilustre isso para as seguintes frases que fazem parte de um mesmo corpus de texto (i.e.: são usadas para a montagem do léxico do modelo). Na sua resposta mostre como ficaria o léxico do modelo e os bag of words correspondentes:\n",
    "    - Frase 1: Eu quero tomar água!\n",
    "    - Frase 2: eu, prefiro tomar café.\n",
    "    \n",
    "**Resposta:** Ao montar um modelo do tipo bag of words sem primeiro tratar os dados (removendo caracteres especiais, convertendo a capitalização das palavras, etc) o risco de ruídos aparecerem entre as palavras aumenta drasticamente, assim dificultando as próximas etapas do processo (como aplicação de TF-IDF e classificações), onde muitas palavras iguais são tratadas como diferentes apenas por estarem minúsculas ou maiúsculas, além de pontuações serem concatenadas, como se pode observar nos exemplos abaixo:\n",
    "\n",
    "| Modelo Léxico |\n",
    "|:-------------:|\n",
    "| `['Eu', 'quero', 'tomar', 'água!', 'eu,', 'prefiro', 'café.']` |\n",
    "\n",
    "| Frase                   | Bag of Words            |\n",
    "| :---------------------- |:------------------------|\n",
    "| Eu quero tomar água!    | `[1, 1, 1, 1, 0, 0, 0]` |\n",
    "| eu, prefiro tomar café. | `[0, 0, 1, 0, 1, 1, 1]` |\n",
    "\n",
    "2. (TC.3.2) Qual a relação entre as etapas de pré-processamento de texto e a redução de dimensionalidade quando se lida com extração de características? Ilustre isso considerando 2 exemplos que façam uso de stemização e/ou lematização.\n",
    "\n",
    "**Resposta:** No pré-processamento os ruídos (como caracteres especiais, stopwords, etc.) são removidos e é aplicada a stemização e/ou lematização. A partir do array das palavras pré-processadas, é realizada a extração de características do mesmo. Por exemplo, nas frases “O cão corre na chuva” e “O cão não corre na chuva”:\n",
    "\n",
    "| LEMATIZAÇÃO                  | STEMMING                  |\n",
    "| :--------------------------- |:--------------------------|\n",
    "| `['cao', 'correr', 'chuva']` | `[ 'ca', 'corr', 'chuv']` |\n",
    "| `[1, 1, 1]`                  | `[1, 1, 1]`               |\n",
    "\n",
    "Como visto acima, a realização correta do pré-processamento antes da extração de características é muito importante, pois dessa forma os dados numéricos gerados são mais coerentes.\n",
    "\n",
    "2. (TC.3.3) Descreva como ficaria o léxico do modelo e o vetor de características n-gram para n=1, 2 e 3 para os seguintes documentos pertencentes ao mesmo corpus:\n",
    "- Frase 1: Eu não gostei do produto e o produto parece ruim.\n",
    "\n",
    "    \n",
    "| Modelo Léxico |\n",
    "|:-------------:|\n",
    "| `[‘Eu’, ‘não’, ‘gostei’, ‘produto’, ‘parece’, ‘ruim’]` |\n",
    "    \n",
    "| Vetor de Características n-gram n=1 | Vetor de Características n-gram n=2 | Vetor de Características n-gram n=3 |\n",
    "|:------------------------------------|:------------------------------------|:------------------------------------|\n",
    "| `[‘Eu’, ‘não’, ‘gostei’, ‘produto’, ‘parece’, ‘ruim’]` | `[‘Eu não’, ‘não gostei’, ‘produto produto’, 'produto parece', ‘parece ruim’]` | `[‘Eu não gostei’, ‘não gostei produto’, ‘gostei produto produto’, ‘produto produto parece’, ‘produto parece ruim’]`|\n",
    "    \n",
    "- Frase 2: O produto parece bom.\n",
    "\n",
    "        \n",
    "| Modelo Léxico |\n",
    "|:-------------:|\n",
    "| `[‘produto’, ‘parece’, ‘bom’]` |\n",
    "    \n",
    "| Vetor de Características n-gram n=1 | Vetor de Características n-gram n=2 | Vetor de Características n-gram n=3 |\n",
    "|:------------------------------------|:------------------------------------|:------------------------------------|\n",
    "| `[‘produto’, ‘parece’, ‘bom’]`      | `[‘produto parece’, ‘parece bom’]`  | `[‘produto parece bom’]`            |\n",
    "    \n",
    "- Frase 3: O produto parece ruim.\n",
    "\n",
    "         \n",
    "| Modelo Léxico |\n",
    "|:-------------:|\n",
    "| `[‘produto’, ‘parece’, ‘ruim’]` |\n",
    "    \n",
    "| Vetor de Características n-gram n=1 | Vetor de Características n-gram n=2 | Vetor de Características n-gram n=3 |\n",
    "|:------------------------------------|:------------------------------------|:------------------------------------|\n",
    "| `[‘produto’, ‘parece’, ‘ruim’]`     | `[‘produto parece’, ‘parece ruim’]` | `[‘produto parece ruim’]`           |\n",
    "\n",
    "2. (TC.3.4) Para o exercício TC.3.3, considerando um modelo bag of words, com n=1, mostre como se calcula o valor da transformação TFIDF para as palavras produto e ruim, ambas na frase 1. Utilize as expressões fornecidas no material das aulas de PLN. \n",
    "\n",
    "**Resposta:** Partindo da frase 1: \"Eu não gostei do produto e o produto parece ruim\", temos os seguintes cálculos:\n",
    "\n",
    "\n",
    "| Modelo Léxico                                                 | Bag of words da frase 1 |\n",
    "|:-------------------------------------------------------------:|:------------------------|\n",
    "| `[‘Eu’, ‘não’, ‘gostei’, ‘produto’, ‘parece’, ‘ruim’, 'bom']` |`[1, 1, 1, 1, 1, 1, 0]`  |\n",
    "\n",
    "- **Cálculos para \"produto\":**\n",
    "    - TF(‘produto’):\n",
    "         > Número de vezes que ‘produto’ aparece na frase = 2 <br> Número de termos na frase = 8 <br> **TF(‘produto’): 2/8 = 0.25**\n",
    "    - IDF(‘produto’): \n",
    "        > Número de documentos = 3 <br> Número de documentos em que o termo aparece: 3 <br> **IDF(‘produto’): log(3/3) = log(1) = 0**\n",
    "    - TF-IDF(‘produto’) = TF(‘produto’) * IDF(‘produto’) = **0.25 * 0 = 0**\n",
    "        \n",
    "        \n",
    "- **Cálculos para \"ruim\":**\n",
    "    - TF(‘ruim’):\n",
    "         > Número de vezes que ‘produto’ aparece na frase = 1 <br> Número de termos na frase = 8 <br> **TF(‘ruim’): 1/8 = 0.1**\n",
    "    - IDF(‘ruim’): \n",
    "        > Número de documentos = 3 <br> Número de documentos em que o termo aparece: 2 <br> **IDF(‘ruim’): log(3/2) = log(3)/2 = 0.2**\n",
    "    - TF-IDF(‘ruim’) = TF(‘ruim’) * IDF(‘ruim’) = **0.1 * 0.2 = 0.02**\n",
    "\n",
    "Dados os cálculos acima, o valor da transformação TFIDF para \"produto\" e \"ruim\" seriam, respectivamente e aproximadamente, 0 e 0.02."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24f8c18",
   "metadata": {
    "papermill": {
     "duration": 0.004182,
     "end_time": "2023-10-25T00:22:14.858281",
     "exception": false,
     "start_time": "2023-10-25T00:22:14.854099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prática de programação\n",
    "---\n",
    "\n",
    "1. (PP.3.1)  Baseando-se nos exemplos fornecidos pelo professor, que fazem uso da biblioteca scikit-learn, ilustre a obtenção de um vetor do tipo bag of words com transformação do tipo TFIDF para dois documentos que representem reviews de produtos em um site de e-commerce. Execute todas as etapas de pré-processamento necessárias para normalizar os dados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c44462",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-25T00:22:14.869557Z",
     "iopub.status.busy": "2023-10-25T00:22:14.869048Z",
     "iopub.status.idle": "2023-10-25T00:22:51.304715Z",
     "shell.execute_reply": "2023-10-25T00:22:51.303007Z"
    },
    "papermill": {
     "duration": 36.445269,
     "end_time": "2023-10-25T00:22:51.307974",
     "exception": false,
     "start_time": "2023-10-25T00:22:14.862705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'pt' are deprecated. Please use the\r\n",
      "full pipeline package name 'pt_core_news_sm' instead.\u001b[0m\r\n",
      "Collecting pt-core-news-sm==3.6.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.6.0/pt_core_news_sm-3.6.0-py3-none-any.whl (13.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /opt/conda/lib/python3.10/site-packages (from pt-core-news-sm==3.6.0) (3.6.1)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.0.4)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.0.9)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.0.7)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.0.8)\r\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (8.1.12)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.1.2)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.4.7)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.0.9)\r\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (0.9.0)\r\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (0.10.2)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (6.3.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (4.66.1)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.23.5)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.31.0)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.10.9)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.1.2)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (68.0.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (21.3)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.3.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.0.9)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (4.6.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2023.7.22)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (0.7.10)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (0.1.1)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (8.1.7)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->pt-core-news-sm==3.6.0) (2.1.3)\r\n",
      "Installing collected packages: pt-core-news-sm\r\n",
      "Successfully installed pt-core-news-sm-3.6.0\r\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\r\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d9bccfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-25T00:22:51.324280Z",
     "iopub.status.busy": "2023-10-25T00:22:51.323774Z",
     "iopub.status.idle": "2023-10-25T00:23:02.051957Z",
     "shell.execute_reply": "2023-10-25T00:23:02.050636Z"
    },
    "papermill": {
     "duration": 10.740258,
     "end_time": "2023-10-25T00:23:02.055093",
     "exception": false,
     "start_time": "2023-10-25T00:22:51.314835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ameir</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.446656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apaixonar</th>\n",
       "      <td>0.333102</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cor</th>\n",
       "      <td>0.333102</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delicioso</th>\n",
       "      <td>0.333102</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fininho</th>\n",
       "      <td>0.333102</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ideal</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.446656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ir</th>\n",
       "      <td>0.333102</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>juro</th>\n",
       "      <td>0.333102</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lir</th>\n",
       "      <td>0.237005</td>\n",
       "      <td>0.317800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pano</th>\n",
       "      <td>0.237005</td>\n",
       "      <td>0.317800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pedir</th>\n",
       "      <td>0.333102</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qualidade</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.446656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tamanho</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.446656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vestir</th>\n",
       "      <td>0.333102</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1\n",
       "ameir      0.000000  0.446656\n",
       "apaixonar  0.333102  0.000000\n",
       "cor        0.333102  0.000000\n",
       "delicioso  0.333102  0.000000\n",
       "fininho    0.333102  0.000000\n",
       "ideal      0.000000  0.446656\n",
       "ir         0.333102  0.000000\n",
       "juro       0.333102  0.000000\n",
       "lir        0.237005  0.317800\n",
       "pano       0.237005  0.317800\n",
       "pedir      0.333102  0.000000\n",
       "qualidade  0.000000  0.446656\n",
       "tamanho    0.000000  0.446656\n",
       "vestir     0.333102  0.000000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nlp = spacy.load(\"pt_core_news_sm\", disable=['parser', 'ner'])\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Remove stopwords\n",
    "    stop_words = [word for word in nlp.Defaults.stop_words]\n",
    "    cleaned_text = \" \".join([i for i in text if i not in set(stop_words)])\n",
    "    return cleaned_text\n",
    "    \n",
    "def clean_text(text):\n",
    "    # Aplica a remoção de stopwords, caracteres não alfabéticos e outras palavras curtas\n",
    "    df_corpus = []\n",
    "    for i in range(len(text)):\n",
    "        df_c = re.sub('[^A-Za-záàâãéèêíïóôõöúçñÁÀÂÃÉÈÍÏÓÔÕÖÚÇÑ]', ' ', text[i]).lower().split()\n",
    "        df_corpus.append(df_c)\n",
    "        \n",
    "    df_corpus= pd.Series(df_corpus).apply(lambda x: ' '.join([w for w in x if len(w)>2]))\n",
    "    corpus = [remove_stopwords(r.split()) for r in df_corpus]\n",
    "    return corpus\n",
    "    \n",
    "def lemmatization(texto):\n",
    "    # Extrai o lema das palavras\n",
    "    global nlp\n",
    "    output = []\n",
    "    for sent in texto:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        output.append([token.lemma_ for token in doc])\n",
    "        \n",
    "    return output\n",
    "    \n",
    "def lemmatize(text):\n",
    "    # Aplica a limpeza do texto e a lemmatização\n",
    "    token = lemmatization(pd.Series(clean_text(text)).apply(lambda x: x.split()))\n",
    "    token_lemma = []\n",
    "    for i in range(len(token)):\n",
    "        token_lemma.append(' '.join(token[i]))\n",
    "    \n",
    "    return token_lemma\n",
    "    \n",
    "\n",
    "documents = [\n",
    "    \"Eu tô apaixonada!!! Pano delicioso (não é fininho!) e vestido lindo demais. Vou pedir em todas as cores, juro!\", \n",
    "    \"é lindo, tamanho ideal, pano bom, eu amei a qualidade\"\n",
    "]\n",
    "\n",
    "corpus = lemmatize(documents)\n",
    "\n",
    "tfidf_vectorized = TfidfVectorizer(min_df=0., max_df=1., norm='l2', use_idf=True)\n",
    "features = tfidf_vectorized.fit_transform(corpus)\n",
    "model_lexicon = tfidf_vectorized.get_feature_names_out()\n",
    "feature_vectors = features.toarray()\n",
    "\n",
    "feature_df_new_model = pd.DataFrame(feature_vectors, columns = model_lexicon).transpose()\n",
    "feature_df_new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac509b",
   "metadata": {
    "papermill": {
     "duration": 0.006543,
     "end_time": "2023-10-25T00:23:02.069271",
     "exception": false,
     "start_time": "2023-10-25T00:23:02.062728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "2. (PP.3.2) Considerando um corpus de texto contendo revisões de produtos, selecione algumas revisões que possam ser caracterizadas como positivas, negativas ou neutras. Treine cada um dos modelos seguintes, tendo como base o código-fonte fornecido pelo professor, para que sejam capazes de classificar uma determinada revisão informada pelo usuário, diferente daquela que foi utilizada no treinamento do modelo. Salve os dados do seu modelo treinado em um arquivo pickle, recarregue e demonstre a sua utilização para nova classificação.\n",
    "        a. Multilayer perceptron;\n",
    "        b. K-Nearest Neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17d2c6b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-25T00:23:02.085602Z",
     "iopub.status.busy": "2023-10-25T00:23:02.084823Z",
     "iopub.status.idle": "2023-10-25T00:23:02.322092Z",
     "shell.execute_reply": "2023-10-25T00:23:02.320565Z"
    },
    "papermill": {
     "duration": 0.24894,
     "end_time": "2023-10-25T00:23:02.325084",
     "exception": false,
     "start_time": "2023-10-25T00:23:02.076144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors  import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "classified_reviews= [\n",
    "    {'corpus': \"usei apenas uma vez mas pareceu ser bom para meu cabelo\", 'review_type': 'positive', 'feature_vector': []},\n",
    "    {'corpus': \"é um bom equipamento, recomendo a todos, o preço está muito bom\", 'review_type': 'positive', 'feature_vector': []},\n",
    "    {'corpus': \"sempre compro na americanas e nunca tiver problemas, recomendo\", 'review_type': 'positive', 'feature_vector': []},\n",
    "    {'corpus': \"as crianças amaram esse brinquedo \", 'review_type': 'positive', 'feature_vector': []},\n",
    "    {'corpus': \"está faltando o DVD \", 'review_type': 'negative', 'feature_vector': []}, \n",
    "    {'corpus': \"péssimo serviço\", 'review_type': 'negative', 'feature_vector': []}, \n",
    "    {'corpus': \"produto veio com defeito \", 'review_type': 'negative', 'feature_vector': []},  \n",
    "    {'corpus': \"não recebi o produto \", 'review_type': 'negative', 'feature_vector': []},\n",
    "    {'corpus': \"ótimo produto mas a entrega atrasou\", 'review_type': 'neutral', 'feature_vector': []},\n",
    "    {'corpus': \"produto atende as necessidades\", 'review_type': 'neutral', 'feature_vector': []},   \n",
    "    {'corpus': \"uma pena que meu cartão não tenha esse limite\", 'review_type': 'neutral', 'feature_vector': []},\n",
    "    {'corpus': \"só vou usar quando estiver no meu novo apartamento\", 'review_type': 'neutral', 'feature_vector': []}         \n",
    "]\n",
    "unclassified_review = {\n",
    "    'corpus': 'ótimo produto mas meu cartão não tem esse limite',\n",
    "    'review_type': '',\n",
    "    'feature_vector': []\n",
    "}\n",
    "\n",
    "base_model_lexicon = ['a', 'ajuda', 'ajudar', 'confusão', 'bom', 'mau', \n",
    "                 'atendimento', 'como', 'confuso', \n",
    "                 'consigo', 'de', 'desejo', 'encerrar', \n",
    "                 'estou', 'favor', 'gostaria', 'há', 'mais',\n",
    "                   'me', 'não', 'obter', 'opção', 'outra', 'poderia', \n",
    "                   'por', 'qual', 'sei', 'é', 'essa']\n",
    "\n",
    "# words used as inputs shall be those resulted from tokenization proccess and other\n",
    "# preprocessing steps.\n",
    "def build_model_lexicon(words, model_lexicon):\n",
    "    for word in words:\n",
    "        if word not in model_lexicon:\n",
    "            model_lexicon.append(word)\n",
    "    model_lexicon.sort()\n",
    "\n",
    "def build_feature_vector(words, model_lexicon):\n",
    "    bag_of_words_count = np.zeros(len(model_lexicon))\n",
    "    for pos in range(len(model_lexicon)):\n",
    "        for word in words:\n",
    "            if word == model_lexicon[pos]:\n",
    "                bag_of_words_count[pos] += 1\n",
    "    return bag_of_words_count\n",
    "\n",
    "\n",
    "# Here we build the model lexicon\n",
    "for classified_review in classified_reviews:\n",
    "    build_model_lexicon(classified_review['corpus'].split(), base_model_lexicon)\n",
    "build_model_lexicon(unclassified_review['corpus'].split(), base_model_lexicon)\n",
    "\n",
    "# Now we extract the feature vector considering the model\n",
    "for classified_review in classified_reviews:\n",
    "    classified_review['feature_vector'] = build_feature_vector(classified_review['corpus'].split(), base_model_lexicon)\n",
    "\n",
    "unclassified_review['feature_vector'] = build_feature_vector(unclassified_review['corpus'].split(), base_model_lexicon)\n",
    "\n",
    "\n",
    "X = [] # feature vectors\n",
    "y = [] # feature classes\n",
    "for review in classified_reviews:\n",
    "    X.append(review['feature_vector'])\n",
    "    y.append(review['review_type'])\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "classifier_model = neigh.fit(X, y)\n",
    "\n",
    "#Salvando modelo em um arquivo pickle\n",
    "pickle.dump(classifier_model,open('text_classifier_model_knn.pkl', 'wb'))\n",
    "\n",
    "#Carregando e fazendo nova classificação\n",
    "knn_model = pickle.load(open('text_classifier_model_knn.pkl' , 'rb'))\n",
    "\n",
    "print(knn_model.predict([unclassified_review['feature_vector']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f601647c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-25T00:23:02.352445Z",
     "iopub.status.busy": "2023-10-25T00:23:02.351720Z",
     "iopub.status.idle": "2023-10-25T00:23:02.456085Z",
     "shell.execute_reply": "2023-10-25T00:23:02.455195Z"
    },
    "papermill": {
     "duration": 0.125181,
     "end_time": "2023-10-25T00:23:02.458676",
     "exception": false,
     "start_time": "2023-10-25T00:23:02.333495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "classified_reviews= [\n",
    "    {'corpus': \"usei apenas uma vez mas pareceu ser bom para meu cabelo\", 'review_type': 'positive', 'feature_vector': []},\n",
    "    {'corpus': \"é um bom equipamento, recomendo a todos, o preço está muito bom\", 'review_type': 'positive', 'feature_vector': []},\n",
    "    {'corpus': \"sempre compro na americanas e nunca tiver problemas, recomendo\", 'review_type': 'positive', 'feature_vector': []},\n",
    "    {'corpus': \"as crianças amaram esse brinquedo \", 'review_type': 'positive', 'feature_vector': []},\n",
    "    {'corpus': \"está faltando o DVD \", 'review_type': 'negative', 'feature_vector': []}, \n",
    "    {'corpus': \"péssimo serviço\", 'review_type': 'negative', 'feature_vector': []}, \n",
    "    {'corpus': \"produto veio com defeito \", 'review_type': 'negative', 'feature_vector': []},  \n",
    "    {'corpus': \"não recebi o produto \", 'review_type': 'negative', 'feature_vector': []},\n",
    "    {'corpus': \"ótimo produto recomendo mas a entrega atrasou\", 'review_type': 'neutral', 'feature_vector': []},\n",
    "    {'corpus': \"produto atende as necessidades\", 'review_type': 'neutral', 'feature_vector': []},   \n",
    "    {'corpus': \"uma pena que meu cartão não tenha esse limite\", 'review_type': 'neutral', 'feature_vector': []},\n",
    "    {'corpus': \"só vou usar no meu novo apartamento\", 'review_type': 'neutral', 'feature_vector': []}         \n",
    "]\n",
    "\n",
    "unclassified_review = {\n",
    "    'corpus': 'ótimo produto, as crianças amaram',\n",
    "    'review_type': '',\n",
    "    'feature_vector': []\n",
    "}\n",
    "\n",
    "base_model_lexicon = ['a', 'ajuda', 'ajudar', 'confusão', 'bom', 'mau', \n",
    "                 'atendimento', 'como', 'confuso', \n",
    "                 'consigo', 'de', 'desejo', 'encerrar', \n",
    "                 'estou', 'favor', 'gostaria', 'há', 'mais',\n",
    "                   'me', 'não', 'obter', 'opção', 'outra', 'poderia', \n",
    "                   'por', 'qual', 'sei', 'é', 'essa']\n",
    "\n",
    "# words used as inputs shall be those resulted from tokenization proccess and other\n",
    "# preprocessing steps.\n",
    "def build_model_lexicon(words, model_lexicon):\n",
    "    for word in words:\n",
    "        if word not in model_lexicon:\n",
    "            model_lexicon.append(word)\n",
    "    model_lexicon.sort()\n",
    "\n",
    "def build_feature_vector(words, model_lexicon):\n",
    "    bag_of_words_count = np.zeros(len(model_lexicon))\n",
    "    for pos in range(len(model_lexicon)):\n",
    "        for word in words:\n",
    "            if word == model_lexicon[pos]:\n",
    "                bag_of_words_count[pos] += 1\n",
    "    return bag_of_words_count\n",
    "\n",
    "\n",
    "# Here we build the model lexicon\n",
    "for classified_review in classified_reviews:\n",
    "    build_model_lexicon(classified_review['corpus'].split(), base_model_lexicon)\n",
    "\n",
    "build_model_lexicon(unclassified_review['corpus'].split(), base_model_lexicon)\n",
    "\n",
    "# Now we extract the feature vector considering the model\n",
    "for classified_review in classified_reviews:\n",
    "    classified_review['feature_vector'] = build_feature_vector(classified_review['corpus'].split(), base_model_lexicon)\n",
    "\n",
    "unclassified_review['feature_vector'] = build_feature_vector(unclassified_review['corpus'].split(), base_model_lexicon)\n",
    "\n",
    "\n",
    "X = [] # feature vectors\n",
    "y = [] # feature classes\n",
    "for review in classified_reviews:\n",
    "    X.append(review['feature_vector'])\n",
    "    y.append(review['review_type'])\n",
    "\n",
    "classifier = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 3), random_state=1)\n",
    "\n",
    "classifier_model = classifier.fit(X, y)\n",
    "\n",
    "\n",
    "#Salvando modelo em um arquivo pickle\n",
    "pickle.dump(classifier_model,open('text_classifier_model_mlp.pkl', 'wb'))\n",
    "\n",
    "#Carregando e fazendo nova classificação\n",
    "mlp_model = pickle.load(open('text_classifier_model_mlp.pkl' , 'rb'))\n",
    "\n",
    "print(mlp_model.predict([unclassified_review['feature_vector']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7043f949",
   "metadata": {
    "papermill": {
     "duration": 0.00744,
     "end_time": "2023-10-25T00:23:02.474832",
     "exception": false,
     "start_time": "2023-10-25T00:23:02.467392",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "3. (PP.3.3) Demonstre a técnica de agrupamento hierárquico de documentos similares utilizando alguns dados de reviews de produtos. Ilustre e explique o dendrograma em especial no que se refere aos pontos de corte para as distâncias. Faça uso de dados de reviews de produtos e não se esqueça de normalizar os dados antes de efetuar a montagem dos vetores de características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa0c35c",
   "metadata": {
    "papermill": {
     "duration": 0.007282,
     "end_time": "2023-10-25T00:23:02.489715",
     "exception": false,
     "start_time": "2023-10-25T00:23:02.482433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "4. (PP.3.4) Demonstre a modelagem de tópicos, com LDA, utilizando alguns documentos representativos de revisões de produtos. Efetue todas as etapas de pré-processamento adequadas antes de efetuar a modelagem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a39bcd9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-25T00:23:02.507877Z",
     "iopub.status.busy": "2023-10-25T00:23:02.507100Z",
     "iopub.status.idle": "2023-10-25T00:23:03.370569Z",
     "shell.execute_reply": "2023-10-25T00:23:03.369442Z"
    },
    "papermill": {
     "duration": 0.875751,
     "end_time": "2023-10-25T00:23:03.373177",
     "exception": false,
     "start_time": "2023-10-25T00:23:02.497426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "{'numa', 'houvermos', 'fomos', 'terei', 'com', 'dele', 'pelas', 'tivéssemos', 'será', 'teve', 'tivessem', 'houverem', 'lhe', 'aos', 'seria', 'das', 'essa', 'que', 'dela', 'eu', 'num', 'teria', 'tiverem', 'teus', 'de', 'fôssemos', 'aquelas', 'esta', 'minha', 'estava', 'forem', 'estou', 'como', 'houveria', 'uma', 'tivermos', 'fôramos', 'estivemos', 'quando', 'houveram', 'suas', 'te', 'estão', 'são', 'estes', 'um', 'nosso', 'sem', 'esses', 'estejamos', 'estivesse', 'aqueles', 'eles', 'seremos', 'houveriam', 'meus', 'estas', 'estivéramos', 'seríamos', 'serei', 'tivesse', 'nossa', 'houverão', 'ao', 'estivessem', 'esteve', 'haja', 'seja', 'tivera', 'nem', 'vos', 'estivermos', 'formos', 'mas', 'do', 'estiver', 'quem', 'as', 'era', 'nos', 'seriam', 'da', 'tenha', 'serão', 'estavam', 'estivéssemos', 'vocês', 'ele', 'for', 'seu', 'no', 'tive', 'minhas', 'seus', 'houvera', 'está', 'nas', 'houvéramos', 'hajamos', 'entre', 'sejamos', 'estivera', 'também', 'temos', 'dos', 'houveríamos', 'sua', 'ou', 'teriam', 'sejam', 'tivemos', 'esteja', 'tem', 'hei', 'tiver', 'tua', 'os', 'já', 'é', 'haver', 'foi', 'estávamos', 'muito', 'terão', 'estive', 'a', 'hajam', 'houver', 'foram', 'estiverem', 'houve', 'mesmo', 'sou', 'houvessem', 'tu', 'aquilo', 'ela', 'eram', 'fosse', 'às', 'qual', 'por', 'na', 'só', 'tenho', 'tuas', 'pelos', 'teu', 'elas', 'estamos', 'tinha', 'para', 'este', 'há', 'hão', 'essas', 'fossem', 'houvéssemos', 'estiveram', 'lhes', 'mais', 'nós', 'nossos', 'o', 'e', 'isto', 'tenhamos', 'houverá', 'havemos', 'pelo', 'tém', 'aquele', 'houverei', 'teremos', 'tenham', 'tivéramos', 'delas', 'tiveram', 'em', 'estejam', 'até', 'esse', 'teríamos', 'meu', 'fui', 'à', 'me', 'somos', 'terá', 'houveremos', 'aquela', 'estar', 'tínhamos', 'isso', 'houvemos', 'deles', 'depois', 'se', 'pela', 'houvesse', 'ser', 'você', 'éramos', 'fora', 'tinham', 'nossas'}\n",
      "================================================================\n",
      "Matriz de Distribuição de Tópicos:\n",
      "    topic 1   topic 2   topic 3\n",
      "0  0.772741  0.112757  0.114502\n",
      "1  0.781992  0.109355  0.108653\n",
      "2  0.120968  0.764430  0.114602\n",
      "3  0.105056  0.788254  0.106690\n",
      "4  0.139892  0.718465  0.141643\n",
      "Tópico:\n",
      "[('comida', 1.0746193082331468), ('não', 0.9914434919803177), ('deliciosa', 0.8529358892791912), ('gostamo', 0.8529358892791912), ('realment', 0.8529358892791912), ('agrad', 0.7295422935756276), ('boa', 0.7295422935756276), ('gosto', 0.7295422935756276), ('celular', 0.3355307765094048), ('danificado', 0.3355307765094048), ('apertado', 0.3352855794653908), ('coub', 0.33495980792540503), ('vestido', 0.33495980792540503), ('bem', 0.33473213775448246), ('certa', 0.33473213775448246), ('medida', 0.33473213775448246)]\n",
      "\n",
      "Tópico:\n",
      "[('coub', 1.1874275867096238), ('vestido', 1.1874275867096238), ('celular', 1.0339404218405932), ('danificado', 1.0339404218405932), ('apertado', 0.9102810357834579), ('bem', 0.8113292408109367), ('certa', 0.8113292408109367), ('medida', 0.8113292408109367), ('não', 0.7844540266176525), ('agrad', 0.33450418074072436), ('boa', 0.33450418074072436), ('gosto', 0.33450418074072436), ('deliciosa', 0.3344804928008152), ('gostamo', 0.3344804928008152), ('realment', 0.3344804928008152), ('comida', 0.33444346919025136)]\n",
      "\n",
      "Tópico:\n",
      "[('não', 0.33881587030272353), ('celular', 0.33763558283654527), ('danificado', 0.33763558283654527), ('apertado', 0.3363848826183275), ('coub', 0.3361150202413792), ('vestido', 0.3361150202413792), ('bem', 0.3360787385178779), ('certa', 0.3360787385178779), ('medida', 0.3360787385178779), ('deliciosa', 0.3359418681895336), ('gostamo', 0.3359418681895336), ('realment', 0.3359418681895336), ('agrad', 0.33580703339986073), ('boa', 0.33580703339986073), ('gosto', 0.33580703339986073), ('comida', 0.3357786559055851)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from pandas import DataFrame\n",
    "\n",
    "# Exemplo de documentos representativos de revisões de produtos\n",
    "documents = [\"Gostamos muito da comida, realmente deliciosa.\",\n",
    "             \"A comida não estava boa; o gosto não agradou.\",\n",
    "             \"O vestido estava muito apertado e não coube.\",\n",
    "             \"O vestido está na medida certa e coube bem.\",\n",
    "             \"O celular está muito danificado\"]\n",
    "\n",
    "# Pré-processamento: Tokenização, remoção de stop words e stemming\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "stop_words.remove('não')\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print('=='*32)\n",
    "print(stop_words)\n",
    "print('=='*32)\n",
    "\n",
    "def preprocess(text):\n",
    "    # Tokenização e conversão para minúsculas\n",
    "    tokens = word_tokenize(text.lower())  \n",
    "    # Stemming\n",
    "    tokens = [ps.stem(token) for token in tokens if token.isalnum()]\n",
    "    # Remoção de stop words  \n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Aplicar pré-processamento aos documentos\n",
    "documents_preprocessed = [preprocess(doc) for doc in documents]\n",
    "\n",
    "\n",
    "# TF-IDF (Avaliação da importancia de palavras / frequencia em um documento)\n",
    "tfidf_vectorized = TfidfVectorizer(min_df=0., max_df=1., norm='l2', use_idf=True)\n",
    "features = tfidf_vectorized.fit_transform(documents_preprocessed)\n",
    "model_lexicon = tfidf_vectorized.get_feature_names_out()\n",
    "\n",
    "\n",
    "# Modelagem de tópicos usando LDA (atribuição de topicos com probabilidade)\n",
    "lda = LatentDirichletAllocation(n_components=3, max_iter=1000, random_state=0)\n",
    "dt_matrix = lda.fit_transform(features)\n",
    "\n",
    "\n",
    "# Exibição dos resultados\n",
    "topic_features = DataFrame(dt_matrix, columns=[\"topic 1\", \"topic 2\", \"topic 3\"])\n",
    "print(\"Matriz de Distribuição de Tópicos:\")\n",
    "print(topic_features)\n",
    "\n",
    "\n",
    "# Exibição dos tópicos mais importantes\n",
    "vocab = model_lexicon\n",
    "topic_matrix = lda.components_\n",
    "for topic_weights in topic_matrix:\n",
    "    topic = [(token, weight) for token, weight in zip(vocab, topic_weights)]\n",
    "    topic = sorted(topic, key=lambda x: -x[1])\n",
    "    topic = [item for item in topic if item[1] > 0.3]  # Ajuste do limiar de importância\n",
    "    print(\"Tópico:\")\n",
    "    print(topic)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 55.55192,
   "end_time": "2023-10-25T00:23:06.576919",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-25T00:22:11.024999",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
